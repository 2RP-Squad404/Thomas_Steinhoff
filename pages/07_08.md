# Relatório de Estudos

**Nome do Estagiário:** Thomas Ventura Steinhoff
**Data:** 07/08/2024

## Módulos/Etapas Feitas:

- Linguagens
- Frameworks
- Apache Beam/Spark
- Google Dataflow

## Resumo dos módulos

Python é uma das linguagens de programação mais utilizadas atualmente, destacando-se pela sua alta flexibilidade e ampla aplicação em diversas áreas, como machine learning, automação, gerenciamento de dados em grande escala e quase qualquer outra área que envolva código. Essa linguagem é amplamente adotada por desenvolvedores experientes e também é muito atrativa para iniciantes, devido à sua sintaxe simples e intuitiva. A linguagem também conta com uma biblioteca repleta de frameworks e ferramentas que facilitam o desenvolvimento em diversas áreas.

Apache¹ Spark é um framework de alta compatibilidade para processamento em larga escala de dados, utilizando distribuição e memória. Sua arquitetura permite a execução rápida de tarefas analíticas, superando soluções tradicionais. Suportando várias linguagens, como Java e Python, e integrando-se com ferramentas de Big Data, o Spark oferece flexibilidade. Além disso, inclui módulos para aprendizado de máquina e processamento em tempo real, como o MLlib e o Spark Streaming, tornando-se uma escolha eficiente para análises de dados.

O Apache Beam é um modelo de programação unificado e open source para processamento de dados em lote e em tempo real. Ele permite definir e executar pipelines² de dados com uma API consistente em diferentes ambientes. Suas principais características incluem várias transformações, suporte a múltiplas linguagens de programação e capacidades avançadas para processamento de eventos em tempo real, como janelas e watermarks, tornando-o ideal para ETL, processamento em tempo real e agregações complexas.

O Google Dataflow é um serviço que executa pipelines de dados criados pelo Apache Beam. Com o processamento de dados em lote e streaming, o Google oferece diversas tecnologias que melhoram a performance do desenvolvedor, como escalabilidade, balanceamento de carga e manutenção automatizados, integração nativa com o Google Cloud, Storage e outros serviços, etc. Em suma, o Dataflow é uma ótima ferramenta fácil de processar e analisar dados de forma eficiente com suas tecnologias automáticas.

Apache Airflow, criado originalmente pela Airbnb, se destaca pelos seus recursos de gestão de fluxos ETL (Extração, Transformação e Carga de Dados). A principal característica do Apache Airflow é a capacidade de definir, programar e monitorar fluxos de trabalho complexos, chamados de DAGs³. Além disso, o Airflow oferece uma interface de usuário baseada na web para visualizar e gerenciar fluxos de trabalho, o que facilita o monitoramento e a depuração de problemas.

1. O Servidor HTTP Apache ou Servidor Apache (Apache HTTP Server) é um servidor web livre criado em 1995 por um grupo de desenvolvedores da NCSA que fornece suporte para uma ampla gama de projetos de software de código aberto.

2. Uma pipeline é uma sequência de etapas ou processos que transformam dados de uma forma para outra. É um fluxo de trabalho organizado onde a saída de uma etapa se torna a entrada da próxima. Muitos projetos populares e inovadores de Big Data, processamento de dados, e outras áreas emergentes de tecnologia foram desenvolvidos sob a tutela da Apache Software Foundation, adotando o prefixo "Apache" em seus nomes.

3. Uma DAG (Directed Acyclic Graph) é um grafo onde as arestas têm uma direção específica e é impossível retornar a um vértice partindo dele mesmo, seguindo as direções das arestas.

Python, Apache Spark, Apache Beam, Google Dataflow e Apache Airflow são ferramentas essenciais no processamento e análise de dados, cada uma oferecendo funcionalidades específicas que atendem a diversas necessidades. Python destaca-se por sua versatilidade e simplicidade, sendo amplamente utilizada para machine learning, automação e análise de dados em grande escala. Apache Spark oferece alta compatibilidade e velocidade no processamento de dados em larga escala, sendo ideal para desenvolvedores que lidam com grandes volumes de dados. Apache Beam proporciona uma abordagem unificada para processamento de dados em lote e em tempo real, suportando múltiplas linguagens e transformações avançadas.

Google Dataflow complementa o Apache Beam com um serviço gerenciado que facilita a execução de pipelines de dados, aproveitando a escalabilidade e o desempenho do Google Cloud. Já o Apache Airflow é essencial na gestão de fluxos de trabalho ETL, permitindo a definição, programação e monitoramento de processos complexos com uma interface de usuário intuitiva. Em conjunto, essas tecnologias oferecem soluções poderosas e flexíveis para enfrentar os desafios do Big Data e da análise de dados em tempo real, proporcionando uma base robusta para o desenvolvimento de sistemas modernos de processamento de dados.

## Links de Laboratórios

**Recursos Utilizados:**

**Principais comandos:**
Python
- 'pip install package_name'
- 'import library_name'
Apache Beam
- 'beam.Map()'
- 'beam.Filter()'
- 'beam.GroupByKey()'

**Desafios Encontrados:**

**Feedback e Ajustes:**

**Próximos Passos:**